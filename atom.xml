<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SmallSquare&#39;s Blog</title>
  
  <subtitle>SmallSquare的博客</subtitle>
  <link href="https://smallsquare.github.io/atom.xml" rel="self"/>
  
  <link href="https://smallsquare.github.io/"/>
  <updated>2023-11-11T22:33:53.695Z</updated>
  <id>https://smallsquare.github.io/</id>
  
  <author>
    <name>SmallSquare</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Some simple understandings of the large language model</title>
    <link href="https://smallsquare.github.io/Large-model/"/>
    <id>https://smallsquare.github.io/Large-model/</id>
    <published>2023-07-15T20:18:39.000Z</published>
    <updated>2023-11-11T22:33:53.695Z</updated>
    
    <content type="html"><![CDATA[<p>The effect of the ChatGPT is stunning enough, GPT4 provides even more accurate and amazing capabilities. The entire IT industry is currently experiencing a shake-up due to the large models. Every company is trying to develop its LLM to keep up with the tech trend.</p><p>It makes me curious: What is the difference between LLM and deep learning that we knew in the past? Here’s some of my rough understanding.</p><span id="more"></span><h1 id="From-the-battle-of-model-structure-to-the-battle-of-dataset-and-training-skill"><a href="#From-the-battle-of-model-structure-to-the-battle-of-dataset-and-training-skill" class="headerlink" title="From the battle of model structure to the battle of dataset and training skill"></a>From the battle of model structure to the battle of dataset and training skill</h1><h2 id="Pretrained-on-HUGE-dataset"><a href="#Pretrained-on-HUGE-dataset" class="headerlink" title="Pretrained on HUGE dataset"></a>Pretrained on HUGE dataset</h2><p>Technically, what changed in this new trend of large models? The answer is obvious. The huge size of models and datasets. </p><p>Here are the model sizes of GPT and other models. It can be found that the amount of parameters in all these models is outrageously large.</p><p>It’s definitely a challenge to collect and preprocess such an amount of data. A good large model must rely on the quantity and quality of input data.</p><table><thead><tr><th></th><th>Amount of params</th><th>Training data</th><th>Release time</th></tr></thead><tbody><tr><td>GPT</td><td>117M</td><td>5GB</td><td>Jun 2018</td></tr><tr><td>GPT2</td><td>Up to 1.5B</td><td>40GB</td><td>Feb 2019</td></tr><tr><td>GPT3</td><td>Up to 175B</td><td>45TB</td><td>May 2020</td></tr><tr><td>GPT3.5</td><td>details not released</td><td>details not released</td><td>Sep 2022</td></tr><tr><td>GPT4</td><td>details not released</td><td>details not released</td><td>Mar 2023</td></tr></tbody></table><table><thead><tr><th></th><th>Amount of params</th><th>Training data</th><th>Release time</th></tr></thead><tbody><tr><td>PaLM</td><td>Up to 540B</td><td>3.6 trillion tokens</td><td>Apr 2022</td></tr><tr><td>GLM</td><td>Up to 130B</td><td>2.5TB</td><td>Aug 2022</td></tr><tr><td>LLaMA</td><td>Up to 65B</td><td>6TB</td><td>Feb 2023</td></tr><tr><td>ERNIE3.0</td><td>over 100B</td><td>details not released</td><td>Mar 2023</td></tr></tbody></table><p>How to get a good performance on a classical deep learning task? People used to focus on the model structure, and there are a bunch of tries of model structures. In the past development of neural network models, researchers have always focused on the improvement of the model structure. But after the famous paper of Transformer, <em>Attention is All You Need</em>, the change of structure gradually stabilised. The attention module is thus suitable for deep networks and makes it possible to make the model size larger and larger.</p><p>The achievement of GPT is no longer based on the improvement of model structure. Actually, the substructure of GPT1, 2, 3, 3.5 and even 4 is the same, just Transformer decoder. The difference is the training method and the size of models and datasets.</p><p>Apart from OpenAI, other tech companies and universities also delving into large models. For example, PaLM, GLM, LLaMa, ERNIE-bot and so on. When looking at these models in detail, the basic structures are not pretty different.</p><p>Model structures are no longer the main battlefield.</p><h2 id="FT-amp-RLHF"><a href="#FT-amp-RLHF" class="headerlink" title="FT &amp; RLHF"></a>FT &amp; RLHF</h2><p>With such a large-scale dataset, <strong>the pretrained model</strong> has learned the patterns of natural language, but this does not mean that the model can understand human commands as well as give appropriate responses. It means that the model can only continue to do sentence completion through the statistical laws from pretrained corpus.</p><p>Therefore, further training is needed. The second stage of training is the Instruction Tuning Stage. The pretrained model has so much knowledge, but it doesn’t know how to answer your question and follow the instructions. This stage is for teaching the model to do so. Technically, it is a kind of SFT(Supervised Fine-Tuning).</p><p>However, the number of SFT dataset in the stage of Instruction Tuning is limited, and also introduce AI hallucinations and other security issues. To solve the problem, reinforcement learning methods have been applied to the field of language models. We need to train a reward model first, and the reward model will rate or rank the answers from the large language model. The reward model should learn the human preference for answers from its training data. That’s why it is called RLHF(Reinforcement Learning from Human Feedback). There are different methods to do so, e.g. BON, DPO and PPO. Researchers are trying various ways to do RLHF. The new challenge is becoming the training skill instead of model structure.</p><h2 id="More"><a href="#More" class="headerlink" title="More"></a>More</h2><p>The better training skill is the reason why OpenAI’s model can stand out. It is crucial, but there are more challenging works to do such as model distillation, multimodal, and inference acceleration… There are many stuff worth exploring in deep learning, and I am optimistic about it.</p><p>These are my simple, childish understandings and opinions of large models. Please correct me if you notice any misinformation! I appreciate that.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[Paper] Attention is All You Need<br><a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a><br>[Paper] GPT-4 Technical Report<br><a href="https://doi.org/10.48550/arXiv.2303.08774">https://doi.org/10.48550/arXiv.2303.08774</a><br>[Paper] Training language models to follow instructions with human feedback<br><a href="https://doi.org/10.48550/arXiv.2203.02155">https://doi.org/10.48550/arXiv.2203.02155</a><br>[Paper] ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation<br><a href="https://doi.org/10.48550/arXiv.2107.02137">https://doi.org/10.48550/arXiv.2107.02137</a><br>[Paper] LLaMA: Open and Efficient Foundation Language Models<br><a href="https://doi.org/10.48550/arXiv.2302.13971">https://doi.org/10.48550/arXiv.2302.13971</a><br>[Paper] Llama 2: Open Foundation and Fine-Tuned Chat Models<br><a href="https://doi.org/10.48550/arXiv.2307.09288">https://doi.org/10.48550/arXiv.2307.09288</a><br>[Paper] GLM: General Language Model Pretraining with Autoregressive Blank Infilling<br><a href="https://doi.org/10.48550/arXiv.2103.10360">https://doi.org/10.48550/arXiv.2103.10360</a><br>[Paper] GLM-130B: An Open Bilingual Pre-trained Model<br><a href="https://doi.org/10.48550/arXiv.2210.02414">https://doi.org/10.48550/arXiv.2210.02414</a><br>[Zhihu] 从零开始训练大模型<br><a href="https://zhuanlan.zhihu.com/p/636270877">https://zhuanlan.zhihu.com/p/636270877</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;The effect of the ChatGPT is stunning enough, GPT4 provides even more accurate and amazing capabilities. The entire IT industry is currently experiencing a shake-up due to the large models. Every company is trying to develop its LLM to keep up with the tech trend.&lt;/p&gt;
&lt;p&gt;It makes me curious: What is the difference between LLM and deep learning that we knew in the past? Here’s some of my rough understanding.&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://smallsquare.github.io/categories/Deep-Learning/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="EN" scheme="https://smallsquare.github.io/tags/EN/"/>
    
  </entry>
  
  <entry>
    <title>如何理解1x1卷积？1x1卷积为什么可以降维？</title>
    <link href="https://smallsquare.github.io/Conv1x1/"/>
    <id>https://smallsquare.github.io/Conv1x1/</id>
    <published>2023-03-17T08:41:17.000Z</published>
    <updated>2023-03-17T10:25:59.341Z</updated>
    
    <content type="html"><![CDATA[<p>1x1卷积就是卷积核大小为1x1的卷积操作。1x1卷积操作的目的很明确：就是变维（这里的维度是指channels维度的数量，而不是增加或者减少特征维度，这个计算机领域的遣词还是要多打磨啊，不然会很误导）。因此1x1卷积也可以被看作一种全连接层，因为达到了类似的操作和类似的效果。这里对1x1操作具体地进行分析，以助理解。</p><span id="more"></span><p>之前看到网上介绍1x1卷积的各种图，感觉都很抽象，每次都要想半天，所以这篇文章好好捋一捋！</p><h1 id="CNN中的channels和多filter"><a href="#CNN中的channels和多filter" class="headerlink" title="CNN中的channels和多filter"></a>CNN中的channels和多filter</h1><p>首先，得先复习一下CNN的一些基础的概念，这样才能更好地理解1x1卷积。</p><p>CNN中的<code>channels</code>有两种（或者说三种）：</p><ul><li><code>in_channels</code>: 输入卷积层的<code>channels</code>数量，取决于上一层的<code>out_channels</code>；如果是第一层，那就是输入图像本身的<code>channels</code>，比如RGB三通道图像。</li><li><code>out_channels</code>: 卷积层输出的<code>channels</code>数量，取决于卷积核(filter)的个数。</li></ul><p>为什么<code>out_channels</code>取决于卷积核的个数呢？<strong>因为一个卷积核本质上会综合全部的输入通道，然后输出一个通道。那么输出了多个通道的特征图的卷积层，本质上是由于多个卷积核各自输出了一张特征图</strong>。因此，有多少个filter就会产生多少个out_channels。这很重要！千万不要在学习的过程中忽略这点！</p><p>也就是说每个卷积核只会产生一张特征图，但是它的信息其实是采样自每一个输入通道的！这里我贴一张引用的别人的博客的图片辅助理解：<br><img src="/./Conv1x1/channels.png" alt="多通道与卷积"></p><h1 id="1x1卷积"><a href="#1x1卷积" class="headerlink" title="1x1卷积"></a>1x1卷积</h1><p>1x1卷积核最早出现在NIN的论文中。有两个用处：①变换维度——其实这里指的是变换了channels维度的数目，并且虽然我们说1x1卷积可以用于降维，但是其实保持channels维度的数目不变或者升高channels维度的数目也是可以的。②增加神经网络的非线性——这样做可以跨通道地进行信息整合。</p><p>那么1x1卷积是如何做到的呢？如下图所示，首先对于一张特征图和一个1x1x1的卷积核来说，并<strong>不会对特征图的width和height产生影响</strong>，所做的操作只是线性变换。<br>而，对于多个channels的输入特征（如图6x6x32），经过一个卷积核(如图1x1x32)的操作之后，就变成了6x6x1的输出特征图；如果此时，假设有<code>m</code>个1x1x32的卷积核进行卷积操作，那么输出的维度就是6x6x<code>m</code>。也就是上面我们复习的，**每个1x1x32的卷积核其实都综合了每个输入通道（总共6x6x32）的信息得到了一张6x6x1的特征图，这<code>m</code>个卷积核的操作加起来，得到了最终的输出——6x6x<code>m</code>**。所以1x1卷积也可以看作类似全连接操作，因为每个卷积核都连接了每个channel，就像全连接层里的一排的任意个数的neuron。<br><img src="/./Conv1x1/1x1_1.webp" alt="1x1卷积"></p><p>这就是为什么1x1卷积可以变维：我们给出几个卷积核，就能得到几个channels的输出特征图；并且这些卷积核会去综合每个channel的信息。<br>这个时候再看这张图，就十分清晰并且可以理解了：<br><img src="/./Conv1x1/dimensions.webp" alt="变维"></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://www.jianshu.com/p/04c2a9ccaa75">卷积神经网络中用1*1卷积有什么作用或者好处呢？ - 简书</a><br><a href="https://arxiv.org/abs/1312.4400">Network in network - arxiv</a><br><a href="https://blog.csdn.net/sscc_learning/article/details/79814146">【CNN】理解卷积神经网络中的通道 channel - CSDN</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;1x1卷积就是卷积核大小为1x1的卷积操作。1x1卷积操作的目的很明确：就是变维（这里的维度是指channels维度的数量，而不是增加或者减少特征维度，这个计算机领域的遣词还是要多打磨啊，不然会很误导）。因此1x1卷积也可以被看作一种全连接层，因为达到了类似的操作和类似的效果。这里对1x1操作具体地进行分析，以助理解。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://smallsquare.github.io/categories/Deep-Learning/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>LaTeX .gitignore file</title>
    <link href="https://smallsquare.github.io/LaTeX-gitignore-file/"/>
    <id>https://smallsquare.github.io/LaTeX-gitignore-file/</id>
    <published>2022-11-08T04:42:01.000Z</published>
    <updated>2023-03-17T08:53:55.167Z</updated>
    
    <content type="html"><![CDATA[<p>When using Git to synchronise LaTeX projects, some files should not be committed. These files could be LaTeX temporary files, your final generated pdf document or hidden files made by OS. Followings may need to be added to your <code>.gitignore</code>.</p><span id="more"></span><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">.texpadtmp</span><br><span class="line">main.pdf</span><br><span class="line">draft*.pdf</span><br><span class="line"></span><br><span class="line">*.synctex(busy)</span><br><span class="line">*.aux</span><br><span class="line">*.bbl</span><br><span class="line">*.blg</span><br><span class="line">*.log</span><br><span class="line">*.synctex.gz</span><br><span class="line">*.fls</span><br><span class="line">*.fdb_latexmk</span><br><span class="line">*.out</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;When using Git to synchronise LaTeX projects, some files should not be committed. These files could be LaTeX temporary files, your final generated pdf document or hidden files made by OS. Followings may need to be added to your &lt;code&gt;.gitignore&lt;/code&gt;.&lt;/p&gt;</summary>
    
    
    
    <category term="LaTeX" scheme="https://smallsquare.github.io/categories/LaTeX/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>PVOD Regression - Photovoltaic power output prediction based on weather data</title>
    <link href="https://smallsquare.github.io/PVOD-regression/"/>
    <id>https://smallsquare.github.io/PVOD-regression/</id>
    <published>2022-07-01T06:31:18.000Z</published>
    <updated>2023-02-27T17:35:46.289Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/yaotc/PVODataset">PVOD</a> is a public dataset that contains real photovoltaic power output and weather condition data. The regression of this dataset can be used to forecast PV power.</p><p>The code is available on my Github repo <a href="https://github.com/SmallSquare/PVOD_Regression">SmallSquare&#x2F;PVOD_Regression</a>. Feel free to correct me😊.</p><span id="more"></span><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Why do this? I planned to use this dataset for my research project, but then my supervisor provided me with a better private dataset for research. So some very simple work done before on this dataset can be shown here as a simple example of data processing and modelling.</p><p>Why does the photovoltaic power output need to be predicted? Unlike conventional power generation, photovoltaic power is very unstable. But it is heavily influenced by day and night and the weather conditions. So, using weather data is a good idea to forecast the PV power.</p><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>This dataset includes data from ten power stations in relatively close geographical proximity. I will use <code>station03</code> as an example.</p><p>We can learn some basic information from the provided metadata. Here are columns of the metadata:</p><blockquote><p>Metadata:</p><ul><li>Station_ID</li><li>Capacity</li><li>PV_Technology</li><li>Panel_size</li><li>Module</li><li>Inverters</li><li>Layout</li><li>Panel_Number</li><li>Array_Tilt</li><li>Pyranometer</li><li>Longitude</li><li>Latitude</li></ul></blockquote><p>Then, we just delve directly into the specific data. <code>station03</code> has 14688 entries. Here are categorized columns of the dataset of <code>station03</code>:</p><blockquote><p>Time:  </p><ul><li>datetime</li></ul><p>Weather:</p><ul><li>nwp_globalirrad</li><li>nwp_directirrad</li><li>nwp_temperature</li><li>nwp_humidity</li><li>nwp_windspeed</li><li>nwp_winddirection</li><li>nwp_pressure</li><li>lmd_totalirrad</li><li>lmd_diffuseirrad</li><li>lmd_temperature</li><li>lmd_pressure</li><li>lmd_winddirection</li><li>lmd_windspeed</li></ul><p>Photovoltaics:</p><ul><li>power</li></ul></blockquote><p>We now have a general view of what the data set looks like. Further, to set the stage for modelling, a correlation analysis is required.</p><p><img src="/PVOD-regression/correlation.png" alt="Correlation Analysis"></p><p>It can be found in the heat map that features of radiation take the highest correlation to the power output. But <strong>we will drop columns of radiation later</strong> because sometimes the radiation is not that easy to measure without a pyranometer and this is almost linearly related to the power output. This may not reflect value in the actual forecast.</p><p>Also, we can observe the distribution of the power output.</p><p><img src="/PVOD-regression/distribution.png" alt="Distribution of PV output"></p><h1 id="Feature-engineering-and-feature-selection"><a href="#Feature-engineering-and-feature-selection" class="headerlink" title="Feature engineering and feature selection"></a>Feature engineering and feature selection</h1><p>The <code>datetime</code> is pretty important and we need to do some engineering on it. For a cyclic feature like date or time, to keep the distance among values in a cyclic feature reasonable, we should encode it. The exact method and rationale can be found in Pierre-Louis Bescond’s blog → <a href="https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca">Cyclical features encoding, it’s about time</a>. His blog is easy to understand and has inspired me well.</p><p>Then, we only select the following locally measured data:<br>(As you can see, <code>datetime</code> now becomes <code>time_sin</code>, <code>time_cos</code>, <code>date_sin</code> and <code>date_cos</code>)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[lmd_temperature&#x27;,</span><br><span class="line">&#x27;lmd_pressure&#x27;,</span><br><span class="line">&#x27;lmd_winddirection&#x27;,</span><br><span class="line">&#x27;lmd_windspeed&#x27;,</span><br><span class="line">&#x27;time_sin&#x27;,</span><br><span class="line">&#x27;time_cos&#x27;,</span><br><span class="line">&#x27;date_sin&#x27;,</span><br><span class="line">&#x27;date_cos&#x27;]</span><br></pre></td></tr></table></figure><p>We can do a new correlation analysis on processed features and target variable <code>power</code>.</p><p><img src="/PVOD-regression/correlation2.png" alt="Correlation Analysis of precessed features"></p><h1 id="Modelling"><a href="#Modelling" class="headerlink" title="Modelling"></a>Modelling</h1><p>Well, now we are finally ready to model. Since this is just a simple try on PVOD, I will use models in <code>scikit-learn</code> package.</p><p>Here is the performance comparison of models. <strong>The following performance is for reference only, as we have not looked carefully for hyperparameters.</strong></p><table><thead><tr><th align="center">Model</th><th align="center">MAE</th><th align="center">MSE</th><th align="center">RMSE</th><th align="center">R-square</th></tr></thead><tbody><tr><td align="center">RF</td><td align="center">0.5162</td><td align="center">1.1332</td><td align="center">1.0646</td><td align="center">0.9524</td></tr><tr><td align="center">GBM</td><td align="center">0.5841</td><td align="center">1.1142</td><td align="center">1.0556</td><td align="center">0.9532</td></tr><tr><td align="center">MLP</td><td align="center">1.1181</td><td align="center">3.7166</td><td align="center">1.9278</td><td align="center">0.8440</td></tr></tbody></table><p>Although we didn’t work carefully on hyperparameters, Random Forest still give us a good prediction.</p><p>The blue line is the prediction, and the black line is the ground truth, we can see how close the prediction of GBM and RF made is to the ground truth.</p><p><img src="/PVOD-regression/prediction.png" alt="Prediction by RF"><br><img src="/PVOD-regression/prediction2.png" alt="Prediction by GBM"></p><h1 id="Feature-importance"><a href="#Feature-importance" class="headerlink" title="Feature importance"></a>Feature importance</h1><p>The RF and GBM can tell the importance of features:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">- RF Feature Importance -</span><br><span class="line">lmd_temperature </span><br><span class="line"> &gt; 9.6774 %</span><br><span class="line">lmd_pressure </span><br><span class="line"> &gt; 3.6530 %</span><br><span class="line">lmd_winddirection </span><br><span class="line"> &gt; 1.0908 %</span><br><span class="line">lmd_windspeed </span><br><span class="line"> &gt; 1.1318 %</span><br><span class="line">time_sin </span><br><span class="line"> &gt; 53.0341 %</span><br><span class="line">time_cos </span><br><span class="line"> &gt; 24.2303 %</span><br><span class="line">date_sin </span><br><span class="line"> &gt; 2.8041 %</span><br><span class="line">date_cos </span><br><span class="line"> &gt; 4.3784 %</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">- GBM Feature Importance -</span><br><span class="line">lmd_temperature </span><br><span class="line"> &gt; 9.3780 %</span><br><span class="line">lmd_pressure </span><br><span class="line"> &gt; 3.5278 %</span><br><span class="line">lmd_winddirection </span><br><span class="line"> &gt; 0.7219 %</span><br><span class="line">lmd_windspeed </span><br><span class="line"> &gt; 0.6464 %</span><br><span class="line">time_sin </span><br><span class="line"> &gt; 53.9076 %</span><br><span class="line">time_cos </span><br><span class="line"> &gt; 23.7690 %</span><br><span class="line">date_sin </span><br><span class="line"> &gt; 2.8473 %</span><br><span class="line">date_cos </span><br><span class="line"> &gt; 5.2021 %</span><br></pre></td></tr></table></figure><p>It can be noticed that features of <code>time</code> are the most important features, as irradiance varies at different times of the day. Apart from this, the <code>temperature</code> is the next most important feature.</p><p><code>Pressure</code> and <code>date</code> also have an impact on the power output, but the influence of <code>windspeed</code> and <code>winddirection</code> are minimal.</p><h1 id="Tail"><a href="#Tail" class="headerlink" title="Tail"></a>Tail</h1><p>As a reminder, this is only a small attempt on PVOD and as such is deficient in many aspects. Also, the code can be found in my repository <a href="https://github.com/SmallSquare/PVOD_Regression">SmallSquare&#x2F;PVOD_Regression</a>. </p><p>Thanks for reading.😉</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>PVOD on Science Data Bank: <a href="https://www.scidb.cn/en/detail?dataSetId=f8f3d7af144f441795c5781497e56b62">PVOD v1.0: A photovoltaic power output dataset - Science Data Bank</a></p><p>PVOD on Github: <a href="https://github.com/yaotc/PVODataset">PVODataset</a></p><p>PVOD on ScienceDirect: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0038092X21008070">A photovoltaic power output dataset: Multi-source photovoltaic power output dataset with Python toolkit</a></p><p><a href="http://dx.doi.org/10.3390/en13102570">Machine Learning Modeling of Horizontal Photovoltaics Using Weather and Location Data</a></p><p><a href="https://towardsdatascience.com/predicting-solar-power-output-using-machine-learning-techniques-56e7959acb1f">Predicting solar power output using machine learning techniques</a></p><p><a href="https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca">Cyclical features encoding, it’s about time!</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://github.com/yaotc/PVODataset&quot;&gt;PVOD&lt;/a&gt; is a public dataset that contains real photovoltaic power output and weather condition data. The regression of this dataset can be used to forecast PV power.&lt;/p&gt;
&lt;p&gt;The code is available on my Github repo &lt;a href=&quot;https://github.com/SmallSquare/PVOD_Regression&quot;&gt;SmallSquare&amp;#x2F;PVOD_Regression&lt;/a&gt;. Feel free to correct me😊.&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://smallsquare.github.io/categories/Machine-Learning/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="EN" scheme="https://smallsquare.github.io/tags/EN/"/>
    
  </entry>
  
  <entry>
    <title>Vision Transformer论文解读</title>
    <link href="https://smallsquare.github.io/Vision-Transformer/"/>
    <id>https://smallsquare.github.io/Vision-Transformer/</id>
    <published>2022-05-20T05:03:55.000Z</published>
    <updated>2023-03-17T08:45:18.484Z</updated>
    
    <content type="html"><![CDATA[<p>发表于ICLR2021的Vision Transformer已经成为后续Transformer模型在CV领域进一步发展的基石，本文为初代Vision Transformer论文<em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>的解读。</p><p>Vision Transformer的pytorch实现可以看我放在Github里的实现：<a href="https://github.com/COMP6248-Reproducability-Challenge/Vision-Transformer-COMP6248CW/blob/main/models.py">Vision Transformer pytorch - Github</a>，欢迎前来star&amp;fork✨✨✨</p><span id="more"></span><h1 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h1><p>首先就是为什么要设计Vision Transformer？过往CV领域基本上主要就是CNN的各种变体的天下，可以说对CNN的依赖非常之深。而由于Transformer在NLP领域的大放异彩，大家都在研究如何使用新的模型代替CNN之前的工作。</p><p><img src="/Vision-Transformer/vit1.png" alt="related work"></p><p>将Transformer移植到CV领域的一个问题是，NLP领域里，一个句子的长度不会长到无法处理，但是CV领域的一张224*224的图片展开就长达50,176，将这样长的序列输入到Transformer里是无法完成的任务😥！将图像展开成一维数组会导致输入长度平方增长，这是将图像输入到Transformer要解决的首要问题。</p><p>所以正如论文里所说，研究者们尝试了许多种不同的方法来解决这个问题，比如仅仅在局部邻域应用注意力机制，亦或是轴注意力等方式减少计算量，除此之外还有将图像拆分成2*2patch的方法（其实这个思路已经近乎等同于Vision Transformer了）等等。</p><h1 id="Structure-of-Vision-Transformer"><a href="#Structure-of-Vision-Transformer" class="headerlink" title="Structure of Vision Transformer"></a>Structure of Vision Transformer</h1><p>下图就是论文中Vision Transformer的结构：</p><p><img src="/Vision-Transformer/ViT.jpg" alt="ViT架构示意图"></p><p>结构解读：</p><p>①Patching：首先Vision Transformer将图片分割成n*n个patch，这样每个patch展开的长度就很大程度地缩小了。e.g.假设图像是224*224大小，设定每个patch的大小为16*16，那么图像就分割成了14*14个patch，并且每个大小为16*16的patch展开之后为256，这个大小就是模型可以处理的了！</p><p>②Position Embedding：如图，patch之后使用一个Linear Projection层将所有patch映射，其实这里就是所有patch过了一个线性层。然后将过了线性层的所有patch进行位置编码，这里一开始我也不知道是怎样生成的位置编码，但其实就是每个位置随机生成的位置编码，并且这个编码是可训练的，后面会进一步说一下位置编码的问题。<br>（另外，在实现中，patching + linear projection的操作可以等价于一个kernel size和stride都等于patch size的Conv2D。这个也很好理解，就是每次卷积核裁出一个patch size的patch，然后再走这么多的步长，就相当于分割了patch，最后卷积层的运算就相当于线性层。所以基本上这步用一个kernel size和stride都等于patch size的Conv2D卷积层就可以做到！）</p><p>③Class Token：这里还有一个cls token，这个方法其实源自Bert模型。根据原文，最后MLP Head分类的时候也只使用cls token中的信息。但是其实使用cls token或者不使用cls token转而对所有Encoder的输出进行全局平均池化，个人认为没有什么太大的区别，因为其实cls token本质就是“窥探”地综合了所有patch的信息。</p><p>④Transformer Encoder：Vision Transformer中其实没有Decoder结构，只使用Encoder。这里我们可以将其与Transformer的结构进行对比来发现Vision Transformer的改进。<br><img src="/Vision-Transformer/transformer.jpg" alt="Transformer架构示意图"><br>其实整体和NLP的原Transformer Encoder的结构非常相似，作者也是尽可能在最小程度修改原模型的程度上将Transformer应用到CV领域来。一个小的区别是两个Layer Norm在Encoder中的位置提前了。<br>输入进入Layer Norm层，后接一个多头自注意力机制，一个残差结构，后面又是一个Layer Norm层，接一个MLP，一个残差结构。这就构成了一个ViT的Encoder Block，然后可以按所需要的网络深度堆叠多个。</p><p>⑤MLP Head：正如前面所说，原文只使用cls token的输出作为MLP Head的输入进行分类。但是使用池化我认为也是可以的，因为无论如何只要MLP Head可以获得到整体的特征信息即可。</p><h1 id="SOTA-inductive-bias-and-training-from-scartch"><a href="#SOTA-inductive-bias-and-training-from-scartch" class="headerlink" title="SOTA, inductive bias and training from scartch"></a>SOTA, inductive bias and training from scartch</h1><p>论文是Google Brain和Google Research的团队做的，计算资源那是相当滴丰富（羡慕羡慕羡慕羡慕好羡慕🤤🤤🤤）。</p><p><img src="/Vision-Transformer/vit2.png" alt="performance"></p><p>可以看出，在大型数据集上预训练的ViT模型可以完爆之前的巨型CNN——ResNet152x4了，效果确实有非常明显地提升，是当时的state of the art了。</p><p>但是代价是什么呢？不同于CNN中的卷积结构，Transformer完全没有inductive bias（归纳偏置）。卷积层本身就具有的translation equivariance（平移等价性）和locality（局部性），Transformer是完全没有的。所以相当于这部分东西就得模型自己去学习😥，这一定程度就是位置编码的工作了。</p><p>论文做了非常多的实验，其中就包括位置编码，比如1D位置编码（也就是上面所说的）、2D位置编码、Rel.位置编码、没有位置编码的对比实验等。最后发现只要有位置编码，无所谓是哪种位置编码的，效果都差不多其实。因为位置编码最后无论如何都能学习出来2D的特征，这也涉及到另一个实验——位置编码的余弦相似度那个实验，见下图中间那个小图：</p><p><img src="/Vision-Transformer/vit3.png" alt="position embedding similarity"><br>（该实验就是计算在模型经过训练之后，不同位置的位置编码之间的余弦相似度。该实验的复现也可见我们的Github仓库：<a href="https://github.com/COMP6248-Reproducability-Challenge/Vision-Transformer-COMP6248CW/blob/main/experiments/pos_emb_vit_b_32.py">Position embedding similarity</a>。另有RGB filter的实验复现（即上图中第一个小图）：<a href="https://github.com/COMP6248-Reproducability-Challenge/Vision-Transformer-COMP6248CW/blob/main/experiments/rgb_emb_vit_b_32.py">RGB filter</a>）</p><p>可以看到，其实最后通过位置编码的学习，可以获得到这部分inductive bias，但与此同时，这需要大量的且更general的图像数据集和大规模的训练支撑，在Google JFT300M这种超超超超大数据集上预训练的效果才是最好的，这意味着这会比CNN结构的网络消耗更多的资源和更多的时间。简而言之，Vision Transformer还是非常难以训练的，因为需要大量的成本去训练。</p><p>后续的另一篇论文<em>Three things everyone should know about Vision Transformers</em>也有更多的实验和研究。与其你在数据集上training from scartch（从0开始训练），不如拿预训练好的模型直接fine-tune（微调）。Training from scartch不仅消耗更多的资源，而且效果还不如在超大训练集上预训练后微调的模型。总的来说，迁移模型然后fine-tune才是你要做的。</p><h1 id="尾巴"><a href="#尾巴" class="headerlink" title="尾巴"></a>尾巴</h1><p>总的来说，Vision Transformer打破了CNN在CV领域的垄断局面，但是也具有非常明显的问题，毕竟工业界不是为了刷点，工业界也需要轻量可部署的模型。但是Vision Transformer也算是一个重要的开端✨，后续产生了非常多的Vision Transformer变体，比如和CNN结合，比如在其他CV任务的应用，效果都非常非常好。</p><p>我写这篇解读距离我亲手搭建已经过去了一段时间，所以难免可能会有一些勘误或者表达不准确的地方，欢迎读者指出，我将感激不尽😊😊😊。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>Vision Transformer: <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p><p>Transformer: <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p><p>Bert: <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p>Three things of ViT: <a href="https://arxiv.org/abs/2203.09795">Three things everyone should know about Vision Transformers</a></p><p>ResNet: <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p><p>BiT: <a href="https://arxiv.org/abs/1912.11370">Big Transfer (BiT): General Visual Representation Learning</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;发表于ICLR2021的Vision Transformer已经成为后续Transformer模型在CV领域进一步发展的基石，本文为初代Vision Transformer论文&lt;em&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/em&gt;的解读。&lt;/p&gt;
&lt;p&gt;Vision Transformer的pytorch实现可以看我放在Github里的实现：&lt;a href=&quot;https://github.com/COMP6248-Reproducability-Challenge/Vision-Transformer-COMP6248CW/blob/main/models.py&quot;&gt;Vision Transformer pytorch - Github&lt;/a&gt;，欢迎前来star&amp;amp;fork✨✨✨&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://smallsquare.github.io/categories/Deep-Learning/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>在Linux上训练深度学习模型常用的一些命令整理</title>
    <link href="https://smallsquare.github.io/Common-commands-when-training-deep-learning-models-on-Linux/"/>
    <id>https://smallsquare.github.io/Common-commands-when-training-deep-learning-models-on-Linux/</id>
    <published>2022-04-22T15:00:53.000Z</published>
    <updated>2023-03-17T08:42:46.780Z</updated>
    
    <content type="html"><![CDATA[<p>最近在用学校的服务器训练一些模型，所以整理几个很常用的命令……<span id="more"></span><br>带有端口转发的ssh，可以方便在Linux服务器上开个Jupyter Notebook在本机连入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -L localhost:[local port]:localhost:[remote port] [username]@[remote address]</span><br></pre></td></tr></table></figure><p>查看训练卡详情，包括卡的温度、实时功率和显存占用等<br>其中<code>watch</code>可以常驻终端动态查看，参数<code>-n</code>可以控制更新频率</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch nvidia-smi</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 0.1 nvidia-smi</span><br></pre></td></tr></table></figure><p>查看系统的整体运行情况<br>进入后按<code>e</code>可以调整下半部分单位，<code>E</code>调整上半部分单位，如k、m、g、t、p</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p>列出conda包含的环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在用学校的服务器训练一些模型，所以整理几个很常用的命令……</summary>
    
    
    
    <category term="Deep Learning" scheme="https://smallsquare.github.io/categories/Deep-Learning/"/>
    
    
    <category term="整理" scheme="https://smallsquare.github.io/tags/%E6%95%B4%E7%90%86/"/>
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>ModuleNotFoundError: No module named &#39;flaxformer&#39;</title>
    <link href="https://smallsquare.github.io/No-module-named-flaxformer/"/>
    <id>https://smallsquare.github.io/No-module-named-flaxformer/</id>
    <published>2022-04-17T17:08:59.000Z</published>
    <updated>2022-06-08T14:31:24.292Z</updated>
    
    <content type="html"><![CDATA[<p>✨Check the date of this article before reading!✨</p><h1 id="No-module-named-‘flaxformer’"><a href="#No-module-named-‘flaxformer’" class="headerlink" title="No module named ‘flaxformer’"></a>No module named ‘flaxformer’</h1><p>Recently, Google used more and more models built on JAX and Flax. When I try to run Vision Transformer pretrained models created by Google, I got the following error. </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named &#x27;flaxformer&#x27;</span><br></pre></td></tr></table></figure><p>But I already installed the <code>requirements.txt</code>, I was confused about the error. And the <code>flaxformer</code> can not be installed by using <code>pip install [package name]</code> since it did not release on PyPI.</p><span id="more"></span><p>Then I checked the repositories of flax and flaxformer:<br><a href="https://github.com/google/flax">https://github.com/google/flax</a><br><a href="https://github.com/google/flaxformer">https://github.com/google/flaxformer</a></p><h1 id="Fix"><a href="#Fix" class="headerlink" title="Fix"></a>Fix</h1><p>The module <code>flaxformer</code> was not released, so it can be installed by:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install git+https://github.com/google/flaxformer</span><br></pre></td></tr></table></figure><p>Solved.🎉</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;✨Check the date of this article before reading!✨&lt;/p&gt;
&lt;h1 id=&quot;No-module-named-‘flaxformer’&quot;&gt;&lt;a href=&quot;#No-module-named-‘flaxformer’&quot; class=&quot;headerlink&quot; title=&quot;No module named ‘flaxformer’&quot;&gt;&lt;/a&gt;No module named ‘flaxformer’&lt;/h1&gt;&lt;p&gt;Recently, Google used more and more models built on JAX and Flax. When I try to run Vision Transformer pretrained models created by Google, I got the following error. &lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ModuleNotFoundError: No module named &amp;#x27;flaxformer&amp;#x27;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;But I already installed the &lt;code&gt;requirements.txt&lt;/code&gt;, I was confused about the error. And the &lt;code&gt;flaxformer&lt;/code&gt; can not be installed by using &lt;code&gt;pip install [package name]&lt;/code&gt; since it did not release on PyPI.&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://smallsquare.github.io/categories/Deep-Learning/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="EN" scheme="https://smallsquare.github.io/tags/EN/"/>
    
  </entry>
  
  <entry>
    <title>Windows11/10快捷键启动缓慢的解决方案 Solution to slow start with hot key on Windows11/10</title>
    <link href="https://smallsquare.github.io/Solution-to-slow-start-with-hot-key-on-Windows11-10/"/>
    <id>https://smallsquare.github.io/Solution-to-slow-start-with-hot-key-on-Windows11-10/</id>
    <published>2022-04-09T14:23:41.000Z</published>
    <updated>2023-03-17T08:46:00.558Z</updated>
    
    <content type="html"><![CDATA[<p>Windows上的快捷启动会变得缓慢。</p><p>Starting with hotkey always run slow on Windows.</p><p>快捷键启动指的是诸如<code>win+shift+数字键</code>和<code>Ctrl+Alt+自定义键</code>的快捷键启动方式，比如自定义<code>Ctrl+Alt+T</code>为Windows Terminal的快捷键。</p><p>Starting with hotkey means hotkeys like <code>win+shift+num</code> and <code>Ctrl+Alt+custom key</code>on Windows. For example, people often set <code>Ctrl+Alt+T</code> to be the hotkey of starting Windows Terminal.</p><span id="more"></span><p><img src="/Solution-to-slow-start-with-hot-key-on-Windows11-10/terminal.png" alt="Windows Terminal"></p><p>但是Windows有个bug会使得快捷键启动反应很慢。</p><p>But there is a bug on Windows making it slow when starting something.</p><h1 id="Windows10"><a href="#Windows10" class="headerlink" title="Windows10"></a>Windows10</h1><p>在Windows10上，进入设置，隐私，后台应用，关闭“设置”的后台权限即可解决。</p><p>On Windows10, you can solve that by going to Settings, Privacy, Background Apps and turning off the permission of “Setting”.</p><h1 id="Windows11"><a href="#Windows11" class="headerlink" title="Windows11"></a>Windows11</h1><p>Windows11中，微软移除了Windows10里的后台应用菜单，但是可以进入应用设置菜单关闭这个选项。在Windows11上，在开始菜单右键设置，进入应用设置，将后台应用权限修改为“从不”。</p><p>In Windows 11, Microsoft removed the background application menu from Windows 10. On Windows 11, now you need to right-click on Settings in the Start menu, go to Application Settings and change the background application permission to “Never”.</p><p><img src="/Solution-to-slow-start-with-hot-key-on-Windows11-10/setting.png" alt="Setting on Windows11"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Windows上的快捷启动会变得缓慢。&lt;/p&gt;
&lt;p&gt;Starting with hotkey always run slow on Windows.&lt;/p&gt;
&lt;p&gt;快捷键启动指的是诸如&lt;code&gt;win+shift+数字键&lt;/code&gt;和&lt;code&gt;Ctrl+Alt+自定义键&lt;/code&gt;的快捷键启动方式，比如自定义&lt;code&gt;Ctrl+Alt+T&lt;/code&gt;为Windows Terminal的快捷键。&lt;/p&gt;
&lt;p&gt;Starting with hotkey means hotkeys like &lt;code&gt;win+shift+num&lt;/code&gt; and &lt;code&gt;Ctrl+Alt+custom key&lt;/code&gt;on Windows. For example, people often set &lt;code&gt;Ctrl+Alt+T&lt;/code&gt; to be the hotkey of starting Windows Terminal.&lt;/p&gt;</summary>
    
    
    
    <category term="Windows" scheme="https://smallsquare.github.io/categories/Windows/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="EN" scheme="https://smallsquare.github.io/tags/EN/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>PyCharm下的Jupyter Notebook输出图片颜色反转的原因</title>
    <link href="https://smallsquare.github.io/pycharm-jupyter-inverse-color/"/>
    <id>https://smallsquare.github.io/pycharm-jupyter-inverse-color/</id>
    <published>2022-04-06T04:51:07.000Z</published>
    <updated>2023-03-17T09:10:31.692Z</updated>
    
    <content type="html"><![CDATA[<p>今天debug两小时不知道为什么输入的图片画出来颜色怪怪的，以为是通道不对，调了半天发现RGB通道根本没问题，最后发现是pycharm会在深色皮肤下默认反转jupyter notebook输出图片的颜色，吐了。</p><span id="more"></span><p><img src="/pycharm-jupyter-inverse-color/1.png" alt="&quot;zombie&quot; flower"></p><p>修改如下设置回归正常颜色。</p><p><img src="/pycharm-jupyter-inverse-color/3.png" alt="setting"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天debug两小时不知道为什么输入的图片画出来颜色怪怪的，以为是通道不对，调了半天发现RGB通道根本没问题，最后发现是pycharm会在深色皮肤下默认反转jupyter notebook输出图片的颜色，吐了。&lt;/p&gt;</summary>
    
    
    
    <category term="Pycharm" scheme="https://smallsquare.github.io/categories/Pycharm/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>解决Powershell阻止脚本运行遇到的问题 Solution to powershell disable runing scripts</title>
    <link href="https://smallsquare.github.io/Powershell-disable-runing-scripts/"/>
    <id>https://smallsquare.github.io/Powershell-disable-runing-scripts/</id>
    <published>2022-03-27T14:03:33.000Z</published>
    <updated>2023-03-17T08:44:53.525Z</updated>
    
    <content type="html"><![CDATA[<p>当使用命令 <code>set-executionpolicy remotesigned</code> 解除Powershell对脚本运行的限制时，目前的用户可能没有被应用新的策略。</p><p>When using the command <code>set-executionpolicy remotesigned</code> to allow scripts running on powershell, current user may not be appled the change.</p><span id="more"></span><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set-executionpolicy : Windows PowerShell 已成功更新你的执行策略，但在更具体的作业域中定义的策略覆盖了该设置。由于发生覆盖，你的外壳程序将保留其当前的有效执行策略 Restricted。请键入“Get-ExecutionPolicy -List”以查看你的执行策略设置。有关详细信息，请参阅“Get-Help Set-ExecutionPolicy”。</span><br><span class="line">所在位置 行:1 字符: 1</span><br><span class="line">+ set-executionpolicy remotesigned</span><br><span class="line">+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">+ CategoryInfo : PermissionDenied: (:) [Set-ExecutionPolicy], SecurityException</span><br><span class="line">+ FullyQualifiedErrorId : ExecutionPolicyOverride,Microsoft.PowerShell.Commands.SetExecutionPolicyCommand</span><br></pre></td></tr></table></figure><p>这时可以使用命令<code>Set-ExecutionPolicy RemoteSigned -Scope CurrentUser</code>来更新当前用户的策略，然后就可以轻松地在powershell运行脚本啦~</p><p>In this case, we can use command <code>Set-ExecutionPolicy RemoteSigned -Scope CurrentUser</code> to update the policy on current user. It will work now.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;当使用命令 &lt;code&gt;set-executionpolicy remotesigned&lt;/code&gt; 解除Powershell对脚本运行的限制时，目前的用户可能没有被应用新的策略。&lt;/p&gt;
&lt;p&gt;When using the command &lt;code&gt;set-executionpolicy remotesigned&lt;/code&gt; to allow scripts running on powershell, current user may not be appled the change.&lt;/p&gt;</summary>
    
    
    
    <category term="Windows" scheme="https://smallsquare.github.io/categories/Windows/"/>
    
    <category term="Powershell" scheme="https://smallsquare.github.io/categories/Windows/Powershell/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="EN" scheme="https://smallsquare.github.io/tags/EN/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>CSGO中常用命令整理 CSGO common commands</title>
    <link href="https://smallsquare.github.io/CSGO-common-commands/"/>
    <id>https://smallsquare.github.io/CSGO-common-commands/</id>
    <published>2022-03-25T06:58:22.000Z</published>
    <updated>2023-07-18T17:27:55.941Z</updated>
    
    <content type="html"><![CDATA[<p>网上有很多整理的，但是很多很实用的命令简中社区查不到，因此整理一份我个人觉得更实用的，长期更新。</p><span id="more"></span><h1 id="最最最基本的指令"><a href="#最最最基本的指令" class="headerlink" title="最最最基本的指令"></a>最最最基本的指令</h1><p><code>connect [ip]:[port] password [pwd]</code> 连接某服务器</p><p><code>disconnect</code> <code>retry</code> 断开连接&#x2F;重连</p><p><code>quit</code> 退出游戏</p><p><code>net_graph 0/1</code> 关闭&#x2F;显示网络状况，如fps、ping、loss、choke、var、tick等</p><p><code>fps_max 400</code> 最高fps</p><h1 id="服务器内的指令"><a href="#服务器内的指令" class="headerlink" title="服务器内的指令"></a>服务器内的指令</h1><h2 id="基本的"><a href="#基本的" class="headerlink" title="基本的"></a>基本的</h2><p><code>mp_restartgame 1</code> 1秒后重开游戏</p><p><code>mp_autoteambalance 0/1</code> 自动平衡关闭&#x2F;开启</p><p><code>mp_maxmoney 16000</code> 最高金钱</p><p><code>mp_startmoney 16000</code> 出生金钱</p><p><code>mp_buy_anywhere 1</code> 任意地点购买</p><p><code>mp_buytime 15</code> 开局购买时间限制</p><p><code>mp_roundtime 60</code> 回合时间，单位分钟</p><p><code>mp_maxrounds 30</code> 最高回合数</p><p><code>mp_freezetime 0</code> 每局出生原地冻结时间0秒</p><p><code>mp_warmup_end</code> 立即结束热身时间</p><p><code>mp_friendlyfire 0/1</code> 关闭&#x2F;开启友伤</p><p><code>mp_limitteams 2</code> 阵营差异</p><p><code>maxplayers 16</code> 最多玩家数</p><p><code>map de_inferno</code> 切换地图(如de_mirage、de_inferno)</p><p><code>maps</code> 列出该服务器所有地图</p><p><code>mp_overtime_enable 0/1</code> 关闭&#x2F;开启加时赛</p><p><code>mp_randomspawn 0/1</code> 关闭&#x2F;开启随机出生点</p><p><code>mp_teammates_are_enemies 1</code> 无阵营区分，适合死斗</p><p><code>exec gamemode_[gamemode name]</code> 更改游戏模式</p><p><code>sv_cheats 0/1</code> 关闭&#x2F;开启作弊功能，部分功能需要此选项开启</p><p><code>sv_gravity 800</code> 重力，默认800</p><p><code>give weapon_[weapon name]</code> 获得武器</p><p><code>god</code> <code>gods</code> 自己&#x2F;所有人godmode</p><p><code>r_drawothermodels 2</code> 透视绘制</p><p><code>noclip</code> 无视墙</p><p><code>sv_infinite_ammo 0/1/2</code> 无限弹药</p><p><code>host_timescale 1.0</code> 时间倍率，用来调速</p><h2 id="Bot相关"><a href="#Bot相关" class="headerlink" title="Bot相关"></a>Bot相关</h2><p><code>bot_add</code> 随机增加一个bot</p><p><code>bot_add_ct</code> 增加一名CT bot</p><p><code>bot_add_t</code> 增加一名T bot</p><p><code>bot_kick</code> 踢出所有bot</p><p><code>bot_kill</code> 杀死所有bot</p><p><code>bot_stop 1</code> bot原地不动</p><p><code>bot_freeze 1</code> 冻结所有bot</p><p><code>bot_difficulty 0/1/2/3</code> 设置bot难度</p><h2 id="语音频道"><a href="#语音频道" class="headerlink" title="语音频道"></a>语音频道</h2><p><code>sv_alltalk 0/1</code> 关闭&#x2F;开启敌方语音频道</p><p>有时需要配合<code>sv_talk_enemy_dead 0/1</code> <code>sv_talk_enemy_living 0/1</code></p><p><code>sv_full_alltalk 0/1</code> 关闭&#x2F;开启所有人互通的语音频道，包括观众</p><p><code>sv_deadtalk 0/1</code> 关闭&#x2F;开启天堂语音频道（死人说话）</p><h1 id="服务器内的进阶指令"><a href="#服务器内的进阶指令" class="headerlink" title="服务器内的进阶指令"></a>服务器内的进阶指令</h1><h2 id="跑图相关"><a href="#跑图相关" class="headerlink" title="跑图相关"></a>跑图相关</h2><p><code>sv_showimpacts 1</code> 显示子弹落点</p><p><code>sv_showimpacts_time 15</code> 子弹落点停留时间</p><p><code>sv_grenade_trajectory 1</code> 显示投掷物轨迹</p><p><code>sv_grenade_trajectory_time 15</code> 投掷物轨迹停留时间</p><h2 id="连跳相关"><a href="#连跳相关" class="headerlink" title="连跳相关"></a>连跳相关</h2><p><code>sv_enablebunnyhopping 1</code> 允许连跳</p><p><code>sv_autobunnyhopping 1;</code> 自动连跳</p><h2 id="弹道扩散相关"><a href="#弹道扩散相关" class="headerlink" title="弹道扩散相关"></a>弹道扩散相关</h2><p><code>weapon_accuracy_nospread 0/1</code> 枪口不扩散，类似跳狙飞人</p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p><code>mp_damage_headshot_only 1</code> 仅限爆头击杀</p><p>本篇将会长期更新。</p><p>针对更多需要，可以查看这个Link <a href="https://totalcsgo.com/commands">CS:GO Commands List</a>.</p><p><img src="/CSGO-common-commands/csgomeme.jpg" alt="CSGO meme"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;网上有很多整理的，但是很多很实用的命令简中社区查不到，因此整理一份我个人觉得更实用的，长期更新。&lt;/p&gt;</summary>
    
    
    
    <category term="Have fun" scheme="https://smallsquare.github.io/categories/Have-fun/"/>
    
    <category term="CSGO" scheme="https://smallsquare.github.io/categories/Have-fun/CSGO/"/>
    
    
    <category term="整理" scheme="https://smallsquare.github.io/tags/%E6%95%B4%E7%90%86/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>安装hexo-asset-image导致Hexo插入图片无法显示的解决办法 Solution to insert pics in Hexo when hexo-asset-image is installed</title>
    <link href="https://smallsquare.github.io/Solution-to-insert-pics-in-Hexo/"/>
    <id>https://smallsquare.github.io/Solution-to-insert-pics-in-Hexo/</id>
    <published>2022-03-23T11:53:22.000Z</published>
    <updated>2023-03-17T08:53:42.585Z</updated>
    
    <content type="html"><![CDATA[<p>请根据本文使用的Hexo版本自测本文时效性。<br>Please check the version of Hexo before reading.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hexo -v</span><br><span class="line">INFO  Validating config</span><br><span class="line">hexo: 6.1.0</span><br><span class="line">hexo-cli: 4.3.0</span><br></pre></td></tr></table></figure><h1 id="NO"><a href="#NO" class="headerlink" title="NO!"></a>NO!</h1><p><del><code>$ npm install hexo-asset-image –save</code></del></p><span id="more"></span><h1 id="正确的做法"><a href="#正确的做法" class="headerlink" title="正确的做法"></a>正确的做法</h1><p>Hexo已经支持Markdown语法插入图片了！！<br>Hexo已经支持Markdown语法插入图片了！！<br>Hexo已经支持Markdown语法插入图片了！！<br>网上说hexo不能用Markdown相对引用图片的资料已经过时了！！<br>*IMPORTANT: Unlike the past, Hexo already supports Markdown syntax for inserting images!</p><p>以下摘自官方：<br>The following is the part of official document:</p><blockquote><p><font size=5>Embedding an image using markdown</font></p><p>hexo-renderer-marked 3.1.0 introduced a new option that allows you to embed an image in markdown without using asset_img tag plugin.</p><p>To enable: </p><blockquote><p><font color="grey">_config.yml</font><br>post_asset_folder: true<br>marked:<br>prependRoot: true<br>postAsset: true</p></blockquote><p>Once enabled, an asset image will be automatically resolved to its corresponding post’s path. For example, “image.jpg” is located at “&#x2F;2020&#x2F;01&#x2F;02&#x2F;foo&#x2F;image.jpg”, meaning it is an asset image of “&#x2F;2020&#x2F;01&#x2F;02&#x2F;foo&#x2F;“ post, <code>![](image.jpg)</code> will be rendered as <code>&lt;img src=&quot;/2020/01/02/foo/image.jpg&quot;&gt;</code>.</p></blockquote><p>所以其实Hexo已经可以不使用繁杂的标签<code>&#123;% asset_img example.jpg This is an example image %&#125;</code>去插入图片了,可以直接使用Markdown语法并进行相对路径引用了，如<code>![](image.jpg)</code>。<br>So actually we don’t need to use the nasty form <code>&#123;% asset_img example.jpg This is an example image %&#125;</code> to insert images. Feel free to use the Markdown way<code>![](image.jpg)</code>.</p><h1 id="错误的做法"><a href="#错误的做法" class="headerlink" title="错误的做法"></a>错误的做法</h1><p>一些网上的资料因为年代久远且被不断翻新会建议使用<code>npm install hexo-asset-image –save</code>命令安装<code>hexo-asset-image</code>进行图片插入。在新版本Hexo不要这样做，否则会导致最终的绝对路径出现bug导致图片无法引用。<br>如果已经执行了，请回退<code>node_modules</code>目录、<code>package.json</code>等文件到之前的版本，并使用<code>hexo clean</code>清除<code>db.json</code>和<code>public</code>目录然后重新生成。</p><p>Some outdated tutorials may tell you that you have to use <code>npm install hexo-asset-image –save</code> to install <code>hexo-asset-image</code> for inserting images. DO NOT DO THAT NOW, cause it will make a bug in generated absolute paths. And the images will be not available in your blogs.<br>If you have already installed it, Please roll back the <code>node_modules</code> directory, <code>package.json</code> and other files to the previous version and use <code>hexo clean</code> to clear the <code>db.json</code> and <code>public</code> directory and then regenerate them.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;请根据本文使用的Hexo版本自测本文时效性。&lt;br&gt;Please check the version of Hexo before reading.&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo -v&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;INFO  Validating config&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo: 6.1.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo-cli: 4.3.0&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h1 id=&quot;NO&quot;&gt;&lt;a href=&quot;#NO&quot; class=&quot;headerlink&quot; title=&quot;NO!&quot;&gt;&lt;/a&gt;NO!&lt;/h1&gt;&lt;p&gt;&lt;del&gt;&lt;code&gt;$ npm install hexo-asset-image –save&lt;/code&gt;&lt;/del&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="https://smallsquare.github.io/categories/Hexo/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="EN" scheme="https://smallsquare.github.io/tags/EN/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>记录一下维护hexo常用的几个命令</title>
    <link href="https://smallsquare.github.io/Common-commands-of-using-hexo/"/>
    <id>https://smallsquare.github.io/Common-commands-of-using-hexo/</id>
    <published>2022-03-20T04:05:38.000Z</published>
    <updated>2023-03-17T08:42:45.453Z</updated>
    
    <content type="html"><![CDATA[<p>整理一部分最常用的hexo命令</p><span id="more"></span><h1 id="维护hexo常用的几个命令"><a href="#维护hexo常用的几个命令" class="headerlink" title="维护hexo常用的几个命令"></a>维护hexo常用的几个命令</h1><p>生成，参数<code>-d</code>可以生成后部署，等效<code>hexo d -g</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo generate</span><br><span class="line"></span><br><span class="line">hexo g -d</span><br><span class="line">hexo d -g</span><br></pre></td></tr></table></figure><p>启动服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br><span class="line">hexo server</span><br></pre></td></tr></table></figure><p>部署，部署前需要正确配置<code>_config.yml</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><p>新建文章</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo n &quot;article&quot;</span><br><span class="line">hexo new &quot;article&quot;</span><br></pre></td></tr></table></figure><p>清除缓存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;整理一部分最常用的hexo命令&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="https://smallsquare.github.io/categories/Hexo/"/>
    
    
    <category term="整理" scheme="https://smallsquare.github.io/tags/%E6%95%B4%E7%90%86/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>LaTeX插入图片及多行多列图片排版</title>
    <link href="https://smallsquare.github.io/LaTeX-insert-images/"/>
    <id>https://smallsquare.github.io/LaTeX-insert-images/</id>
    <published>2022-03-18T14:01:08.000Z</published>
    <updated>2023-03-17T08:44:06.095Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LaTeX插入一张图片"><a href="#LaTeX插入一张图片" class="headerlink" title="LaTeX插入一张图片"></a>LaTeX插入一张图片</h1><p>首先导入宏包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;graphicx&#125;</span><br></pre></td></tr></table></figure><p>插入图片</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">    \centering</span><br><span class="line">    \includegraphics[width=0.6\textwidth]&#123;figures/image.png&#125;</span><br><span class="line">    \caption&#123;题注&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><span id="more"></span><p>其中，<code>[htbp]</code>是控制图片位置的参数</p><table><thead><tr><th align="center">param</th><th align="center">position</th></tr></thead><tbody><tr><td align="center">h</td><td align="center">当前位置</td></tr><tr><td align="center">t</td><td align="center">页面顶部</td></tr><tr><td align="center">b</td><td align="center">页面底部</td></tr><tr><td align="center">p</td><td align="center">浮动</td></tr></tbody></table><h1 id="插入并列的两张独立图片"><a href="#插入并列的两张独立图片" class="headerlink" title="插入并列的两张独立图片"></a>插入并列的两张独立图片</h1><p>使用<code>minipage</code>分栏实现并列图片插入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">    \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125;</span><br><span class="line">        \centering</span><br><span class="line">        \includegraphics[width=0.8\textwidth]&#123;figures/image1.png&#125;</span><br><span class="line">        \caption&#123;题注1&#125;</span><br><span class="line">    \end&#123;minipage&#125;%</span><br><span class="line">    \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125;</span><br><span class="line">        \centering</span><br><span class="line">        \includegraphics[width=0.8\textwidth]&#123;figures/image2.png&#125;</span><br><span class="line">        \caption&#123;题注2&#125;</span><br><span class="line">    \end&#123;minipage&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>效果如下<br><img src="/LaTeX-insert-images/%E5%B9%B6%E5%88%972.png" alt="并列效果图"></p><h1 id="插入并列的两张不独立的图片"><a href="#插入并列的两张不独立的图片" class="headerlink" title="插入并列的两张不独立的图片"></a>插入并列的两张不独立的图片</h1><p>直接连续使用两次<code>\includegraphics[]&#123;&#125;</code>即可，<code>\hspace&#123;&#125;</code>表示间距。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">    \centering</span><br><span class="line">    \includegraphics[width=0.4\textwidth]&#123;figures/image1.png&#125;</span><br><span class="line">    \hspace&#123;0.1in&#125;</span><br><span class="line">    \includegraphics[width=0.4\textwidth]&#123;figures/image2.png&#125;</span><br><span class="line">    \caption&#123;Result&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>效果如下<br><img src="/LaTeX-insert-images/%E5%B9%B6%E5%88%971.png" alt="并列效果图"></p><h1 id="插入多行多列，其中每列算一个独立图片"><a href="#插入多行多列，其中每列算一个独立图片" class="headerlink" title="插入多行多列，其中每列算一个独立图片"></a>插入多行多列，其中每列算一个独立图片</h1><p>写Report的时候突然有这样一个需求，就是实现下图这种每列算一个独立图片的多行多列图片。每个独立图片里面其实有两张图片。</p><p><img src="/LaTeX-insert-images/%E5%A4%9A%E8%A1%8C%E5%A4%9A%E5%88%972.png" alt="多行多列效果图"></p><p>使用<code>minipage</code>分栏，然后使用<code>\\</code>实现图片换行竖排显示。<br>具体实现办法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">\centering</span><br><span class="line">\begin&#123;minipage&#125;[t]&#123;0.3\linewidth&#125;</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width=4cm]&#123;figures/image11.png&#125; \\</span><br><span class="line">        \includegraphics[width=4cm]&#123;figures/image12.png&#125;</span><br><span class="line">        \caption&#123;\\1&#125;</span><br><span class="line">\end&#123;minipage&#125;</span><br><span class="line">\begin&#123;minipage&#125;[t]&#123;0.3\linewidth&#125;</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width=4cm]&#123;figures/21.png&#125; \\</span><br><span class="line">        \includegraphics[width=4cm]&#123;figures/22.png&#125;</span><br><span class="line">        \caption&#123;\\2&#125;</span><br><span class="line">\end&#123;minipage&#125;</span><br><span class="line">\begin&#123;minipage&#125;[t]&#123;0.3\linewidth&#125;</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width=4cm]&#123;figures/31.png&#125; \\</span><br><span class="line">        \includegraphics[width=4cm]&#123;figures/32.png&#125;</span><br><span class="line">        \caption&#123;\\3&#125;</span><br><span class="line">\end&#123;minipage&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">\centering</span><br><span class="line">\begin&#123;minipage&#125;[t]&#123;0.3\linewidth&#125;</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width=4cm]&#123;figures/41.png&#125; \\</span><br><span class="line">        \includegraphics[width=4cm]&#123;figures/42.png&#125;</span><br><span class="line">        \caption&#123;\\4&#125;</span><br><span class="line">\end&#123;minipage&#125;</span><br><span class="line">\begin&#123;minipage&#125;[t]&#123;0.3\linewidth&#125;</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width=4cm]&#123;figures/51.png&#125; \\</span><br><span class="line">        \includegraphics[width=4cm]&#123;figures/52.png&#125;</span><br><span class="line">        \caption&#123;\\5&#125;</span><br><span class="line">\end&#123;minipage&#125;</span><br><span class="line">\begin&#123;minipage&#125;[t]&#123;0.3\linewidth&#125;</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width=4cm]&#123;figures/61.png&#125; \\</span><br><span class="line">        \includegraphics[width=4cm]&#123;figures/62.png&#125;</span><br><span class="line">        \caption&#123;\\6&#125;</span><br><span class="line">\end&#123;minipage&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><h1 id="多行多列子图"><a href="#多行多列子图" class="headerlink" title="多行多列子图"></a>多行多列子图</h1><p>使用<code>/subfigure</code>可以划分子图，具体做法先不写了，可以先参考一下这篇<a href="https://blog.csdn.net/itnerd/article/details/105199014">latex 排列多个子图</a>，以后可能填坑。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;LaTeX插入一张图片&quot;&gt;&lt;a href=&quot;#LaTeX插入一张图片&quot; class=&quot;headerlink&quot; title=&quot;LaTeX插入一张图片&quot;&gt;&lt;/a&gt;LaTeX插入一张图片&lt;/h1&gt;&lt;p&gt;首先导入宏包&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;\usepackage&amp;#123;graphicx&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;插入图片&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;\begin&amp;#123;figure&amp;#125;[htbp]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    \centering&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    \includegraphics[width=0.6\textwidth]&amp;#123;figures/image.png&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    \caption&amp;#123;题注&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;\end&amp;#123;figure&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="LaTeX" scheme="https://smallsquare.github.io/categories/LaTeX/"/>
    
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>之前的几篇水博 Previous blogs</title>
    <link href="https://smallsquare.github.io/Previous-articles/"/>
    <id>https://smallsquare.github.io/Previous-articles/</id>
    <published>2019-12-31T16:00:00.000Z</published>
    <updated>2023-03-17T08:46:00.553Z</updated>
    
    <content type="html"><![CDATA[<p>鄙人不才，一共也没写过几篇博客也没啥质量，哈哈哈哈哈哈🤣🤣🤣。但是还是想把东西都放一起，所以放在这儿了。以前还学习过Android开发，现在退坑了。</p><p>I haven’t written a few blogs in total (xDDDDD🤣🤣🤣), but I still wanted to collect them together here. No longer messing with Android now👋👋👋.</p><span id="more"></span><h2 id="Links"><a href="#Links" class="headerlink" title="Links:"></a>Links:</h2><p><a href="https://blog.csdn.net/SmallSquare/article/details/86773697">Android开发-向ViewPager动态添加时报错java.lang.IllegalStateException</a></p><p><a href="https://blog.csdn.net/SmallSquare/article/details/86773634">Android开发-Gson解析数据</a></p><p><a href="https://blog.csdn.net/SmallSquare/article/details/86669254">Android开发-RecyclerView简单实现对Item的事件响应以及交互效果</a></p><p><a href="https://blog.csdn.net/SmallSquare/article/details/79506097">Raspberry Pi 3B音频口底噪太大的解决办法</a></p><p><a href="https://blog.csdn.net/SmallSquare/article/details/79332832">Realtek 8822be无线网卡在Ubuntu16&#x2F;17上驱动问题的完美解决方案</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;鄙人不才，一共也没写过几篇博客也没啥质量，哈哈哈哈哈哈🤣🤣🤣。但是还是想把东西都放一起，所以放在这儿了。以前还学习过Android开发，现在退坑了。&lt;/p&gt;
&lt;p&gt;I haven’t written a few blogs in total (xDDDDD🤣🤣🤣), but I still wanted to collect them together here. No longer messing with Android now👋👋👋.&lt;/p&gt;</summary>
    
    
    
    <category term="CSDN" scheme="https://smallsquare.github.io/categories/CSDN/"/>
    
    
    <category term="整理" scheme="https://smallsquare.github.io/tags/%E6%95%B4%E7%90%86/"/>
    
    <category term="技术" scheme="https://smallsquare.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="中文" scheme="https://smallsquare.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
  </entry>
  
</feed>
